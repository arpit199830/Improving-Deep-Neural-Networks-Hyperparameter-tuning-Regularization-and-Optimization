1. a[8]{8}(7)

2. One iteration of mini-batch gradient descent (computing on a single mini-batch) is faster than one iteration of batch gradient descent.

3. If the mini-batch size is m, you end up with batch gradient descent, which has to process the whole training set before making progress.
   
   If the mini-batch size is 1, you lose the benefits of vectorization across examples in the mini-batch.

4. If you’re using mini-batch gradient descent, this looks acceptable. But if you’re using batch gradient descent, something is wrong.

5. V2 = 7.5, V2(corrected)= 10

6. alpha = e^t * alpha0

7. Increasing \betaβ will shift the red line slightly to the right.

   Decreasing \betaβ will create more oscillation within the red line.

8. (1) is gradient descent. (2) is gradient descent with momentum (small \betaβ). (3) is gradient descent with momentum (large \betaβ)

9. Try mini-batch gradient descent

   Try using Adam

   Try tuning the learning rate \alphaα  

   Try better random initialization for the weights
 
10. Adam should be used with batch gradient computations, not with mini-batches.
