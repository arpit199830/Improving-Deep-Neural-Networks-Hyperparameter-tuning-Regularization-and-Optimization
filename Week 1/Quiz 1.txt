1. 98%train , 1% dev, 1% test

2. Come from the same distribution

3. Add regularistaion 
   Get more data

4. Increase the regularistaion parameter lambda
   Get more data

5. A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration.

6. Weights are pushed toward becoming smaller (closer to 0)

7. You do not apply dropout (do not randomly eliminate units) and do not keep the 1/keep_prob factor in the calculations used in training

8. Reducing the regularisation effect.
   Causing the neural network to end up with a lower training set error

9. Dropout
   L2 Regularisation
   Data Augmentation

10. It makes it easier to visualise the data